{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scipy\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "%pip install biosppy\n",
    "%pip install peakutils\n",
    "%pip install --upgrade setuptools pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "from math import *\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from biosppy.signals import ecg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filepath\n",
    "filepath = 'PTB_processed_new'\n",
    "completed = 0\n",
    "total = len(os.listdir(filepath))\n",
    "# Load label file\n",
    "labels_file = pd.read_csv(\"new_labels_processed.csv\", header = 0)\n",
    "labels = np.asarray(labels_file['diagnosis'])\n",
    "# Create empty containers\n",
    "signal_array = [] # To store all segments\n",
    "label_array = [] # TO store all labels\n",
    "# Create counters\n",
    "no_segment = 0\n",
    "files_completed = 0\n",
    "file_number = 0\n",
    "# Signal Extractor\n",
    "start_time = time.time()\n",
    "for filename in os.listdir(filepath):\n",
    "    start_time_file = time.time()\n",
    "    data = pd.read_csv(filepath + '/' + filename, header = 0, engine = 'python')\n",
    "    file_label = labels[file_number]\n",
    "    df = np.array(data['v4'])\n",
    "    peaks = ecg.christov_segmenter(df, 1000)[0]\n",
    "    for peak in range (1, len(peaks) - 1):\n",
    "        segment_array = [] # To store 1 segment\n",
    "        for column in data:\n",
    "            df = np.array(data[column])\n",
    "            segment = df[peaks[peak]-50:peaks[peak]+100]\n",
    "            segment_array.append(segment)\n",
    "        signal_array.append(segment_array)\n",
    "        label_array.append(file_label)\n",
    "        no_segment += 1\n",
    "    files_completed += 1\n",
    "    file_number += 1\n",
    "    progress = files_completed / total * 100\n",
    "    progress = round(progress, 2)\n",
    "    end_time_file = time.time()\n",
    "    elasped_time_file = round(end_time_file - start_time_file, 2)\n",
    "    sys.stdout.write('\\r'+filename + ' loaded - ' + str(progress) + '% completed' + ' - Took: ' + str(elasped_time_file) + ' seconds')\n",
    "\n",
    "\n",
    "signal_array = np.asarray(signal_array).reshape(-1, 150, 12)\n",
    "end_time = time.time()\n",
    "elasped_time = round(end_time - start_time, 2)\n",
    "print('')\n",
    "print ('Data loading completed ' + str(files_completed) + ' files loaded with ' + str(no_segment) + ' signal segments' + ' - Took: ' + str(elasped_time) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(label_array)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(signal_array, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Flatten the data for classification (since RandomForestClassifier does not support 3D input)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train a classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_flat, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_flat)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
